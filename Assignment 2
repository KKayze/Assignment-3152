rm(list=ls())
library(ROCR)
library(pROC)
library(readr)
library(dplyr)
library(rpart)
library(neuralnet)
library(carData)
library(car)
WAUS = read.csv("Desktop/FIT3152/Assignment 2/HumidPredict2023D.csv")
L = as.data.frame(c(1:49))
set.seed(32810806) # Your Student ID is the random seed
L = L[sample(nrow(L), 10, replace = FALSE),] # sample 10 locations
WAUS = WAUS[(WAUS$Location %in% L),]
WAUS = WAUS[sample(nrow(WAUS), 2000, replace = FALSE),] # sample 2000 rows
View(WAUS)

# ==================================================================================================
#
#                                       Question 1
#
# Explore the data: What is the proportion of days when it is more humid than the previous day 
# compared to those where it is less humid? Obtain descriptions of the predictor (independent) 
# variables – mean, standard deviations, etc. for real-valued attributes. Is there anything 
# noteworthy in the data? Are there any attributes you need to consider omitting from your analysis?
#
# ==================================================================================================

#check the structure
str(WAUS) 

# check the range 
summary(WAUS)

# Check the proportion of 0 and 1 in MHT columns
MHT_prop = as.data.frame(xtabs(~ MHT, WAUS))
cbind(MHT_prop, Percent=prop.table(MHT_prop$Freq)*100)

# Check the proportion of Yes and No in RainToday columns
raintdy_prop = as.data.frame(xtabs(~ RainToday, WAUS))
cbind(raintdy_prop, Percent=prop.table(raintdy_prop$Freq)*100)


#------------------------------------------------------------------------------#
# 1. The 0 shows it will not be more humid and 1 will be more humid tomorrow   #
# 2. The proportion of it will be more humid tomorrow is higher                #
# 3. The proportion of days it is more humid than the previous day is 47.02%   #
# 4. The proportion of days it is less humid than the previous day is 52.98%   #
#------------------------------------------------------------------------------#

# Check the sum of NA values for each column
sapply(WAUS, function(col) sum(is.na(col)) ) 

# Calculate the standard deviation for each column
Standard_Deviation = sapply(WAUS[,-c(4,10,12:13,24:25)], sd, na.rm = T)
Standard_Deviation

# Calculate the mean for MHT column
mean_MHT = apply(WAUS[,c("MHT"), drop=F], 2, mean, na.rm=T) 
mean_MHT


# ==================================================================================================
#
#                                       Question 2
#
# Document any pre-processing required to make the data set suitable for the model fitting 
# that follows.
#
# ==================================================================================================

# Omitting the NA values
WAUS<-na.omit(WAUS)
summary(WAUS)


# ==================================================================================================
#
#                                       Question 3
#
# Divide your data into a 70% training and 30% test set by adapting the following code 
# (written for the iris data). Use your student ID as the random seed.
#
# ==================================================================================================
#Student ID as random seed 
set.seed(32810806) 

# Split into 70% and 30%
train.row = sample(1:nrow(WAUS), 0.7*nrow(WAUS)) 

# Assign 70% to train data
WAUS.train = WAUS[train.row,]

# Assign 30% to test data
WAUS.test = WAUS[-train.row,]

# Omitting the NA values
WAUS.train<- na.omit(WAUS.train)
WAUS.test<- na.omit(WAUS.test)
# ==================================================================================================
#
#                                       Question 4
#
# Implement a classification model using each of the following techniques. For this question you 
# may use each of the R functions at their default settings if suitable.
#
# ==================================================================================================

#===============#
# Decision Tree #
#===============#
library(tree)
set.seed(32810806)
WAUS.train$MHT <- factor(WAUS.train$MHT)
DT.fit = tree(MHT ~ ., data=WAUS.train)

# Omitting the NA values
DT.fit<- na.omit(DT.fit)

summary(DT.fit)

#===============#
#  Naive Bayes  #
#===============#
library(e1071)
set.seed(32810806)
WAUS.train$MHT <- factor(WAUS.train$MHT)
NB.fit = naiveBayes(MHT ~ ., data=WAUS.train)

# Omitting the NA values
NB.fit<- na.omit(NB.fit)

summary(NB.fit)

#===============#
#    Bagging    #
#===============#
library(adabag)
set.seed(32810806)
WAUS.train$MHT <- factor(WAUS.train$MHT)
Bagging.fit = bagging(MHT ~ ., data=WAUS.train, mfinal=5)

# Omitting the NA values
Bagging.fit<- na.omit(Bagging.fit)

summary(Bagging.fit)

#===============#
#    Boosting   #
#===============#
library(adabag)
set.seed(32810806)
WAUS.train$MHT <- factor(WAUS.train$MHT)
Boosting.fit <- boosting(MHT ~ ., data = WAUS.train, mfinal = 10)

# Omitting the NA values
Boosting.fit<- na.omit(Boosting.fit)

summary(Boosting.fit)

#===============#
# Random Forest #
#===============#
library(randomForest)
set.seed(32810806)
RF.fit = randomForest(MHT ~ ., data=WAUS.train, mfinal = 15)

# Omitting the NA values
RF.fit<- na.omit(RF.fit)

summary(RF.fit)

# ==================================================================================================
#
#                                       Question 5
#
# Using the test data, classify each of the test cases as ‘more humid tomorrow’ or ‘less humid 
# tomorrow’. Create a confusion matrix and report the accuracy of each model.
#
# ==================================================================================================
#===============#
# Decision Tree #
#===============#
DT_confusion_matrix<- table(actual = WAUS.test$MHT, predicted = predict(DT.fit, WAUS.test, type = "class"))
DT_confusion_matrix

DT.acc <- (round(sum(diag(DT_confusion_matrix)) / sum(DT_confusion_matrix), 4)) * 100
DT.acc

#===============#
#  Naive Bayes  #
#===============#
NB_confusion_matrix <- table(actual = WAUS.test$MHT, predicted = predict(NB.fit, WAUS.test))
NB_confusion_matrix

NB.acc <- (round(sum(diag(NB_confusion_matrix)) / sum(NB_confusion_matrix), 4)) * 100
NB.acc

#===============#
#    Bagging    #
#===============#
Bagging.pred <- predict.bagging(Bagging.fit, WAUS.test)
Bagging_confusion_matrix<- Bagging.pred$confusion
Bagging_confusion_matrix

Bagging.acc<- (round(sum(diag(Bagging_confusion_matrix)) / sum(Bagging_confusion_matrix), 4)) * 100
Bagging.acc

#===============#
#    Boosting   #
#===============#
Boosting.pred <- predict.boosting(Boosting.fit, WAUS.test)
Boosting_confusion_matrix<- Boosting.pred$confusion
Boosting_confusion_matrix

Boosting.acc <- (round(sum(diag(Boosting_confusion_matrix)) / sum(Boosting_confusion_matrix), 4)) * 100
Boosting.acc

#===============#
# Random Forest #
#===============#
RF_confusion_matrix <- table(actual = WAUS.test$MHT, predicted = predict(RF.fit, WAUS.test))
RF_confusion_matrix

RF.acc <- (round(sum(diag(RF_confusion_matrix)) / sum(RF_confusion_matrix), 4)) * 100
RF.acc

# Accuracy = 61.07

# ==================================================================================================
#
#                                       Question 6
#
# Using the test data, calculate the confidence of predicting ‘more humid tomorrow’ for each case 
# and construct an ROC curve for each classifier. You should be able to plot all the curves on 
# the same axis. Use a different colour for each classifier. Calculate the AUC for each classifier.
#
# ==================================================================================================

DT_ROC <- predict(DT.fit, newdata = WAUS.test)
NB_ROC <- predict(NB.fit, newdata = WAUS.test, type = "raw")
Bagging_ROC <- predict(Bagging.fit, newdata = WAUS.test)
Boosting_ROC <- predict(Boosting.fit, newdata = WAUS.test)
RF_ROC <- predict(RF.fit, newdata = WAUS.test, type = "prob")

model_classifier = c("Decision Tree", "Naive Bayes", "Bagging", "Boosting", "Random Forest")
classifier_colors = c("red", "blue", "green","black", "yellow")
predicting = cbind(DT_ROC, NB_ROC, Bagging_ROC, Boosting_ROC, RF_ROC)
auc = c()
abline(0, 1)

for (i in 1:5) {
  pred = prediction(predicting[,i*2], WAUS.test$MHT)
  plot(performance(pred, "tpr", "fpr"), 
       add=(i!=1), col=classifier_colors[i], main="ROC Curve") # ROC
  cauc = performance(pred, "auc") # AUC
  cauc2 = round(as.numeric(cauc@y.values), 4) # Round up to 4 decimal places
  auc = append(auc, cauc2)
}
legend("bottomright", model_classifier, col=classifier_colors)

# ==================================================================================================
#
#                                       Question 7
#
# Create a table comparing the results in Questions 5 and 6 for all classifiers. Is there a 
# single “best” classifier?
#
# ==================================================================================================
acc = c(DT.acc, NB.acc, Bagging.acc, Boosting.acc, RF.acc)
summary_table = rbind(Accuracy=acc, AUC=auc)
colnames(summary_table) = model_classifier
summary_table

# ==================================================================================================
#
#                                       Question 8
# Examining each of the models, determine the most important variables in predicting whether it 
# will be more humid tomorrow or not. Which variables could be omitted from the data with very 
# little effect on performance? Give reasons.
#
# ==================================================================================================
#===============#
# Decision Tree #
#===============#
summary(DT.fit)

#===============#
#  Naive Bayes  #
#===============#
# Cannot determine the importance of variables used in Naive Bayes as it only calculates the probabilities of each variable given

#===============#
#    Bagging    #
#===============#
sort(Bagging.fit$importance, decreasing=T)

#===============#
#    Boosting   #
#===============#
sort(Boosting.fit$importance, decreasing=T)

#===============#
# Random Forest #
#===============#
RF.fit$importance[order(-RF.fit$importance),]


# ==================================================================================================
#
#                                       Question 9
#
# Starting with one of the classifiers you created in Question 4, create a classifier that is 
# simple enough for a person to be able to classify whether it will be more humid tomorrow or not 
# by hand. Describe your model, either with a diagram or written explanation. What factors were 
# important in your decision? State why you chose the attributes you used. Using the test data 
# created in Question 3, evaluate model performance using the measures you calculated for 
# Questions 5 and 6. How does it compare to those in Question 4? 
#
# ==================================================================================================
#===============#
# Decision Tree #
#===============#
set.seed(32810806)
plot(DT.fit) 
text(DT.fit, pretty=0)

# Generate attributes for observation use
test_DT.fit = cv.tree(DT.fit, FUN = prune.misclass)
test_DT.fit

# Choose the smallest $dev and $k for pruning process
pruned_DT.fit = prune.misclass(DT.fit, best = 2)
summary(pruned_DT.fit) 

#=============================================================================#
# You would get k = 0 and it simply means model performance won't get changed 
# although the tree is simplified into one with 2 leaf nodes in the end.
#=============================================================================#
plot(pruned_DT.fit) 
text(pruned_DT.fit, pretty=0)
# confusion matrix
pruned_DT.predict = predict(pruned_DT.fit, WAUS.test, type="class") 
pruned_DT_matrix = table(actual=WAUS.test$MHT, predicted=pruned_DT.predict)
pruned_DT_matrix 

# Calculate accuracy
pruned_DT.acc = round(sum(diag(pruned_DT_matrix)) / sum(pruned_DT_matrix), 4) *100
pruned_DT.acc

# Summary table
DT_new_accuracy = c(pruned_DT.acc)
DT_summary_table = rbind(Initial_Accuracy=acc[c(1)], Improved_Accuracy=DT_new_accuracy)
colnames(DT_summary_table) = model_classifier[c(1)]
DT_summary_table

# Accuracy = 60.4

# ==================================================================================================
#
#                                       Question 10
# 
# Create the best tree-based classifier you can. You may do this by adjusting the parameters, 
# and/or cross-validation of the basic models in Question 4. Show that your model is better than 
# the others using the measures you calculated for Questions 5 and 6. Describe how you created your 
# improved model, and why you chose that model. What factors were important in your decision? State 
# why you chose the attributes you used.
#
# ==================================================================================================
#===============#
# Random Forest #
#===============#
set.seed(32810806)

# Cross validation for random forest
RF_cv = rfcv(trainx=WAUS.train[,c(1:11)], trainy=WAUS.train[,c(13)], step=0.8)
RF_cv$error.cv
# When doing sampling, the number of variables = 10 or 8 has the lowest error rate (0.1570) 

# Fit random forest model to train data
set.seed(32810806)

# Confusion matrix
new_RF_pred = predict(RF.fit, WAUS.test)
new_RF_matrix = table(actual=WAUS.test$MHT, predicted=new_RF_pred) 
new_RF_matrix

# Calculate accuracy
new_RF.acc= round(sum(diag(new_RF_matrix))/sum(new_RF_matrix), 4) *100
new_RF.acc

# Accuracy = 61.07

# Generate summary table for comparison purpose
RF_new_accuracy = c(new_RF.acc)
RF_summary_table = rbind(Initial_Accuracy=acc[c(5)], Improved_Accuracy=RF_new_accuracy)
colnames(RF_summary_table) = model_classifier[c(5)]
RF_summary_table

# ==================================================================================================
#
#                                       Question 11
#
# Using the insights from your analysis so far, implement an Artificial Neural Network classifier 
# and report its performance. Comment on attributes used and your data pre- processing required.
# How does this classifier compare with the others? Can you give any reasons?
#
# ==================================================================================================
options(digits = 4)
new_WAUS<-WAUS
new_WAUS$MHT = recode(new_WAUS$MHT," '1' = '1';'0' = '0' ")
new_WAUS$MHT<-as.numeric(new_WAUS$MHT)

set.seed(32810806) 
train.row = sample(1:nrow(new_WAUS), 0.7*nrow(new_WAUS))
new_WAUS.train = new_WAUS[train.row,]
new_WAUS.test = new_WAUS[-train.row,]
CalMHT<-MHT==1 ~MinTemp + MaxTemp + Rainfall + Evaporation + Sunshine + WindGustSpeed + WindSpeed9am + WindSpeed3pm + Cloud9am + Cloud3pm + Temp9am + Temp3pm + RISK_MM

set.seed(32810806) 
new_WAUS.NN = neuralnet(CalMHT,new_WAUS.train,hidden=3,linear.output = FALSE)

cal_cols <- c("MinTemp", "MaxTemp", "Rainfall", "Evaporation", "Sunshine", "WindGustSpeed",
                   "WindSpeed9am", "WindSpeed3pm", "Cloud9am", "Cloud3pm", "Temp9am", "Temp3pm",
                   "RISK_MM")

new_WAUS.pred = compute(new_WAUS.NN, new_WAUS.test[c(cal_cols)])
probability<-as.numeric(new_WAUS.pred$net.result)
predicted<-ifelse(probability>0.5, 1, 0)
table.ANN<-table(observed = new_WAUS.test$MHT, predicted = predicted)
table.ANN

accuracy.NN <- sum(diag(table.ANN)/sum(table.ANN)) *100
accuracy.NN

new_WAUS.auc <- roc(new_WAUS.test$MHT,probability)$auc
new_WAUS.auc

classifier_names <- c("Decision Tree", "Naive Bayes", "Bagging", "Boosting", "Random Forest","rpart decison tree","neural network")
accuracy_comparison = c(DT.acc, NB.acc, Bagging.acc, Boosting.acc, RF.acc)
comparison.acc = round(sum(diag(accuracy.NN))/sum(accuracy.NN))*100
comparison.acc

# ==================================================================================================
#
#                                       Question 12
#
# Fit a new classifier to the data, test and report its performance in the same way as for 
# previous models. You can choose a new type of classifier not covered in the course, or a new 
# version of any of the classifiers we have studied. Either way, you will be implementing a new 
# R package. As a starting point, you might refer to James et al. (2021), or look online. When 
# writing up, state the new classifier and package used. Include a web link to the package details. 
# Give a brief description of the model type and how it works. Comment on the performance of 
# your new model. 
#
# ==================================================================================================

# installing required package
install.packages('e1071')
library(e1071)

# train SVM model
svm_model <- svm(MHT ~ ., data = WAUS.train, kernel = "radial")

# Predict on the test data
svm_predictions <- predict(svm_model, WAUS.test)

# Create a confusion matrix
svm_matrix = table(Predicted = svm_predictions, Actual = WAUS.test$MHT)
print(svm_matrix)

# Calculate the accuracy
svm_accuracy = sum(diag(svm_matrix))/sum(svm_matrix)
print(paste('Accuracy:', round(svm_accuracy, 4)*100))

# Compute the AUC score
svm_pred_num <- ifelse(svm_predictions == "1", 1, 0)
svm_auc = roc(WAUS.test$MHT, svm_pred_num)$auc
print(paste('AUC:', round(svm_auc, 4)))
